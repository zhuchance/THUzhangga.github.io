<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ANN,MATLAB,">










<meta name="description" content="[TOC]">
<meta name="keywords" content="ANN,MATLAB">
<meta property="og:type" content="article">
<meta property="og:title" content="Short Course on neural networks (Tsinghua)">
<meta property="og:url" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/index.html">
<meta property="og:site_name" content="Ga&#39;s blog">
<meta property="og:description" content="[TOC]">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1547625548015.png">
<meta property="og:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1547625647685.png">
<meta property="og:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1547625676222.png">
<meta property="og:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1548328459796.png">
<meta property="og:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1548918199065.png">
<meta property="og:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1548918124718.png">
<meta property="og:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1554651616646.png">
<meta property="og:updated_time" content="2019-04-07T15:40:29.042Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Short Course on neural networks (Tsinghua)">
<meta name="twitter:description" content="[TOC]">
<meta name="twitter:image" content="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1547625548015.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/">





  <title>Short Course on neural networks (Tsinghua) | Ga's blog</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ga's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">发现生活中的美好</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/16/Short-Course-on-neural-networks-Tsinghua/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ga">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ga's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Short Course on neural networks (Tsinghua)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-16T17:10:13+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]<br><a id="more"></a></p>
<h2 id="OpenTA"><a href="#OpenTA" class="headerlink" title="OpenTA"></a>OpenTA</h2><h3 id="Quiz"><a href="#Quiz" class="headerlink" title="Quiz"></a>Quiz</h3><h4 id="Analysis-Taylor-expansion"><a href="#Analysis-Taylor-expansion" class="headerlink" title="Analysis: Taylor expansion"></a>Analysis: Taylor expansion</h4><p>The Taylor expansion of an analytic function around $x_0$ is given by $f ( x ) = \sum _ { n = 0 } ^ { \infty } \frac { 1 } { n ! } f ^ { ( n ) } \left( x _ { 0 } \right) \left( x - x _ { 0 } \right) ^ { n }$, where $f ^ { ( n ) } \left( x _ { 0 } \right)$ denotes the $n ^ { t h }$ derivative of $f$ at $x_0$.</p>
<ul>
<li><p>Taylor-expand the function $f(x)=(1-x)^{-m}$ up to second order ($n=2$) around $x_0=0$, where $m \in \,\mathbb{R}$ is:</p>
<p>$1 + m \cdot x + \frac { m \cdot ( m + 1 ) } { 2 } \cdot x ^ { 2 }$</p>
</li>
<li><p><img src="/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1547625548015.png" alt="">$1 - m \cdot x + \frac { m ^ { 2 } } { 2 } \cdot x ^ { 2 }$</p>
</li>
<li><p><img src="/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1547625647685.png" alt=""></p>
<p>$x - 1 - \frac { 1 } { 2 } \cdot ( x - 1 ) ^ { 2 }$</p>
</li>
<li><p><img src="/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1547625676222.png" alt="1547625676222"></p>
<p>0</p>
<p>Ref:<a href="https://math.stackexchange.com/questions/1246136/maclaurin-series-expansion-for-e-1-x2" target="_blank" rel="noopener">https://math.stackexchange.com/questions/1246136/maclaurin-series-expansion-for-e-1-x2</a></p>
</li>
</ul>
<h4 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h4><ul>
<li>Matrix determinant:</li>
</ul>
<script type="math/tex; mode=display">
A = \left[ \begin{array} { c c } { 0 } & { i } \\ { - i } & { 0 } \end{array} \right]</script><p>$det(A)=-1$ (note: $i^2=-1$)</p>
<ul>
<li>Matrix exponential</li>
</ul>
<p>$e^{i\theta A}=$</p>
<script type="math/tex; mode=display">
\left[ \begin{array} { c c } { \cos ( \theta ) } & { - \sin ( \theta ) } \\ { \sin ( \theta ) } & { \cos ( \theta ) } \end{array} \right]</script><p>Solution: <a href="https://math.stackexchange.com/questions/773359/eulers-identity-in-matrix-form" target="_blank" rel="noopener">https://math.stackexchange.com/questions/773359/eulers-identity-in-matrix-form</a></p>
<script type="math/tex; mode=display">
e ^ { \mathbb { I } \theta } =R ( \theta ) = \left( \begin{array} { c c } { \cos \theta } & { - \sin \theta } \\ { \sin \theta } & { \cos \theta } \end{array} \right) (\mathbb { I } = \left( \begin{array} { c c } { 0 } & { - 1 } \\ { 1 } & { 0 } \end{array} \right))</script><h3 id="Homework"><a href="#Homework" class="headerlink" title="Homework"></a>Homework</h3><h4 id="3D-Boolean-functions"><a href="#3D-Boolean-functions" class="headerlink" title="3D Boolean functions"></a>3D Boolean functions</h4><p>In <a href="https://en.wikipedia.org/wiki/Mathematics" target="_blank" rel="noopener">mathematics</a> and <a href="https://en.wikipedia.org/wiki/Logic" target="_blank" rel="noopener">logic</a>, a <strong>(finitary) Boolean function</strong> (or switching function) is a <a href="https://en.wikipedia.org/wiki/Function_(mathematics" target="_blank" rel="noopener">function</a>) of the form $f: \mathbf{B}^k\rightarrow \mathbf{B}$, where B= {0, 1} is a <em>Boolean domain</em> and <em>k</em> is a non-negative integer called the <a href="https://en.wikipedia.org/wiki/Arity" target="_blank" rel="noopener">arity</a> of the function. In the case where <em>k</em> = 0, the “function” is essentially a constant element of <strong>B</strong>.</p>
<ul>
<li>How many 3-dimensional Boolean functions are there? </li>
</ul>
<p>For $k$ dimensional inputs, there are $2^{ 2^ k }$ Boolean functions. The number of inputs are $2^k​$, for each input, the output can be 0/1.</p>
<ul>
<li><p><strong>Count the number of symmetries of 3-dimensional Boolean functions that map 4 of the possible input patterns to 1</strong>.</p>
<p><strong>ANS</strong>: 6. All other patterns can be mapped by rotation or reflection. The reflection may not only reflect by the facet of axes, but by the facet of diagonals.</p>
<p><img src="/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1548328459796.png" alt=""></p>
</li>
</ul>
<p>Symmetry: can be mapped to each other by rotation and/or reflection.</p>
<ul>
<li><strong>How many linearly separable 3-dimensional Boolean functions are there</strong>? </li>
</ul>
<p>If the pattern contains facet that have crossed diagonals by the back and white balls, it is not linear separable. For example, the left down pattern in the figure above is not linearly separable. </p>
<p>By checking whether is pattern is like ‘0101xxxx’ or 15 others (12 from each facet of the cube, 4 from the diagonals), we get the answer is 104.</p>
<p>The code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_patterns</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Get the all the patterns of 3D boolean functions</span></span><br><span class="line">    pattern_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span> ** <span class="number">8</span>):</span><br><span class="line">        pattern = <span class="string">'0'</span> * (<span class="number">8</span> - len(bin(i)[<span class="number">2</span>:])) + bin(i)[<span class="number">2</span>:]</span><br><span class="line">        pattern_list.append(pattern)</span><br><span class="line">    <span class="keyword">return</span> pattern_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pattern_like</span><span class="params">(pattern,feature)</span>:</span></span><br><span class="line">    <span class="comment"># feature pattern is the pattern like '0101xxxx'</span></span><br><span class="line">    <span class="keyword">for</span> i, f <span class="keyword">in</span> enumerate(feature):</span><br><span class="line">        <span class="keyword">if</span> f != <span class="string">'x'</span>:</span><br><span class="line">            <span class="keyword">if</span> pattern[i] != feature[i]:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">pattern_list = get_all_patterns()</span><br><span class="line">XOR_features = [<span class="string">'0101xxxx'</span>, <span class="string">'1010xxxx'</span>, <span class="string">'01xx10xx'</span>, <span class="string">'10xx01xx'</span>, <span class="string">'x10xx01x'</span>, <span class="string">'x01xx10x'</span>, <span class="string">'xxxx0101'</span>, <span class="string">'xxxx1010'</span>, <span class="string">'1xx00xx1'</span>, <span class="string">'0xx11xx0'</span>, <span class="string">'xx01xx10'</span>, <span class="string">'xx10xx01'</span>, <span class="string">'1x0x0x1x'</span>, <span class="string">'0x1x1x0x'</span>, <span class="string">'x1x0x0x1'</span>, <span class="string">'x0x1x1x0'</span>, <span class="string">'10xxxx10'</span>, <span class="string">'01xxxx01'</span>, <span class="string">'xx1010xx'</span>, <span class="string">'xx0101xx'</span>, <span class="string">'1xx0x01x'</span>, <span class="string">'0xx1x10x'</span>, <span class="string">'x10x0xx1'</span>, <span class="string">'x01x1xx0'</span>]</span><br><span class="line">xor_count = <span class="number">0</span> <span class="comment"># number of not linear separable boolean functions</span></span><br><span class="line"><span class="keyword">for</span> pattern <span class="keyword">in</span> pattern_list:</span><br><span class="line">    flag = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> XOR_features:</span><br><span class="line">        <span class="keyword">if</span> pattern_like(pattern, feature):</span><br><span class="line">            xor_count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">print(len(pattern_list) - xor_count)</span><br></pre></td></tr></table></figure>
<h4 id="Linear-separability-of-4-dimensional-Boolean-functions"><a href="#Linear-separability-of-4-dimensional-Boolean-functions" class="headerlink" title="Linear separability of 4-dimensional Boolean functions"></a>Linear separability of 4-dimensional Boolean functions</h4><p>Describe: 4 inputs, 1 output.</p>
<script type="math/tex; mode=display">
O ^ { ( \mu ) } = \tanh \left[ \frac { 1 } { 2 } \left( \sum _ { i = 1 } ^ { 4 } w _ { i } x _ { i } ^ { ( \mu ) } - \theta \right) \right]</script><script type="math/tex; mode=display">
H = \frac { 1 } { 2 } \sum _ { \mu } \left( t ^ { ( \mu ) } - O ^ { ( \mu ) } \right) ^ { 2 }</script><script type="math/tex; mode=display">
g(x)=\tanh(x/2)</script><script type="math/tex; mode=display">
\delta w_i=-\eta\frac{\partial H}{\partial w_i}=\eta \sum_{\mu}(t^{(\mu)}-O^{(\mu)})x_i^{(\mu)}g'(\sum_{j}(w_j x_j^{(\mu)}-\theta))</script><script type="math/tex; mode=display">
\delta\theta=-\eta \sum_{\mu}(t^{(\mu)}-O^{(\mu)})g'(-\theta)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>) <span class="comment"># make the random number repeatable</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The Neuron Node Class.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.input_node_list = [] <span class="comment"># Inputs of the node</span></span><br><span class="line">        self.output_vector = [] <span class="comment"># Output vector</span></span><br><span class="line">        self.threshold = <span class="number">0.0</span></span><br><span class="line">        self.weight = <span class="number">1.0</span></span><br><span class="line">        self.input_vector = [] <span class="comment"># Input vector</span></span><br><span class="line">        self.activation_func = <span class="keyword">None</span> <span class="comment"># Activation function</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_output</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.output_vector = []</span><br><span class="line">        <span class="keyword">for</span> mu <span class="keyword">in</span> range(len(self.input_node_list[<span class="number">0</span>].input_vector)):</span><br><span class="line">            <span class="comment"># for each pattern</span></span><br><span class="line">            O = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> input_node <span class="keyword">in</span> self.input_node_list:</span><br><span class="line">                O += input_node.input_vector[mu] * input_node.weight</span><br><span class="line">            self.output_vector.append(self.activation_func(O - self.threshold))</span><br><span class="line">        <span class="keyword">return</span> self.output_vector</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputNode</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Input Node Class</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        Node.__init__(self)</span><br><span class="line">        self.activation_func = <span class="keyword">lambda</span> x:x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OutputNode</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Output Node Class</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        Node.__init__(self)</span><br><span class="line">        self.activation_func = <span class="keyword">lambda</span> x: np.tanh(x/<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.eta = <span class="number">0.02</span></span><br><span class="line">        self.input_list = []</span><br><span class="line">        self.output = OutputNode()</span><br><span class="line">        self.target = []</span><br><span class="line">        self.loss_function = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(self)</span>:</span></span><br><span class="line">        delta_w_vector = []</span><br><span class="line">        <span class="keyword">for</span> input_node_i <span class="keyword">in</span> self.input_list:</span><br><span class="line">            delta_wi = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> mu <span class="keyword">in</span> range(len(self.input_list[<span class="number">0</span>].input_vector)):</span><br><span class="line">                b_mu = <span class="number">0.0</span></span><br><span class="line">                <span class="keyword">for</span> input_node_j <span class="keyword">in</span> self.input_list:</span><br><span class="line">                    b_mu += input_node_j.weight * input_node_j.input_vector[mu]</span><br><span class="line">                b_mu -= self.output.threshold</span><br><span class="line">                delta_wi += self.eta * (self.target[mu] - self.output.output_vector[mu]) * input_node_i.input_vector[mu] * (<span class="number">1</span> - np.tanh(b_mu / <span class="number">2.0</span>) ** <span class="number">2</span>) * <span class="number">0.5</span></span><br><span class="line">            delta_w_vector.append(delta_wi)</span><br><span class="line">            <span class="comment"># update weight</span></span><br><span class="line">        <span class="keyword">for</span> i, input_node_i <span class="keyword">in</span> enumerate(self.input_list):</span><br><span class="line">            input_node_i.weight += delta_w_vector[i]</span><br><span class="line">        <span class="comment"># Then update threshold at the output</span></span><br><span class="line">        delta_theta = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> mu <span class="keyword">in</span> range(len(self.input_list[<span class="number">0</span>].input_vector)):</span><br><span class="line">            delta_theta += self.eta * (self.target[mu] - self.output.output_vector[mu]) * (<span class="number">1</span> - np.tanh(self.output.threshold / <span class="number">2</span>) ** <span class="number">2</span>) / <span class="number">2.0</span></span><br><span class="line">        self.output.threshold -= delta_theta</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_loss_function</span><span class="params">(self)</span>:</span></span><br><span class="line">        H = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> mu <span class="keyword">in</span> range(len(self.input_list[<span class="number">0</span>].input_vector)):</span><br><span class="line">            H += (self.target[mu] - self.output.output_vector[mu]) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line">        self.loss_function = H</span><br><span class="line"></span><br><span class="line">perceptron = Perceptron()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    input_node = Node()</span><br><span class="line">    input_node.weight = np.random.uniform(<span class="number">-0.2</span>, <span class="number">0.2</span>, <span class="number">1</span>)</span><br><span class="line">    perceptron.input_list.append(input_node)</span><br><span class="line">    perceptron.output.input_node_list.append(input_node)</span><br><span class="line"></span><br><span class="line">perceptron.output.threshold = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/input_data_numeric.csv'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> data:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> data:</span><br><span class="line">        line = line.split(<span class="string">','</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            perceptron.input_list[i].input_vector.append(int(line[i+<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">A = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>]</span><br><span class="line">B = [<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>]</span><br><span class="line">C = [<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>]</span><br><span class="line">D = [<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>]</span><br><span class="line">E =  [<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>]</span><br><span class="line">F =  [<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">perceptron.target = C</span><br><span class="line">epoch = <span class="number">10</span>**<span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">    perceptron.output.calc_output()</span><br><span class="line">    perceptron.gradient_descent()</span><br><span class="line">    perceptron.calc_loss_function()</span><br><span class="line">    <span class="keyword">if</span> i % int(epoch/<span class="number">10</span>**<span class="number">2</span>) == <span class="number">0</span>:</span><br><span class="line">        print(i, perceptron.loss_function)</span><br></pre></td></tr></table></figure>
<p>After $10^5$ updates, example results:</p>
<p>Target A: loss function: 4.000; output: [0.99933648,0.99933648,0.99933648,4.27588495e-05,0.99933648,0.99933648,4.27588495e-05,0.99933648,4.27588495e-05,0.99933648,4.27588495e-05,4.27588495e-05,0.99933648,4.27588495e-05,4.27588495e-05,4.27588495e-05]</p>
<p><img src="/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1548918199065.png" alt=""></p>
<p>Target D: loss function: 0.07137127; output:</p>
<p>[-0.9983045970305993,0.8467004934374702,-0.9999998804920586,-0.9999825727485746,-0.8473914883414136,-0.9983045970305993,-0.7797995354533719,0.99829629025566,-0.999999998772594,-0.999988364023615,-0.9983045970305993,-0.9999825727485746,-0.8473914883414136,0.8467004934374702,-0.9999998804920586,-0.9983045970305993]</p>
<p><img src="/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1548918124718.png" alt=""></p>
<h4 id="True-False-Questions"><a href="#True-False-Questions" class="headerlink" title="True-False Questions"></a>True-False Questions</h4><ul>
<li>Two hidden layers are necessary to approximate any real valued-function with <em>N</em> inputs and one output in terms of a perceptron.</li>
</ul>
<p><strong>False</strong>.  In general, for $N$ inputs, two hidden layers are sufficient, with $2N​$ units in the first layer, and one unit per basis function in second layer. <strong>Yet it is not always necessary to use two layers for real-valued functions. For continuous functions, one hidden layer is sufficient.</strong> This is ensured by the universal approximation theore. This theorem says any continuous function can be approximated to arbitrary accuracy by a network with a single hidden layer, for sufficiently many neurons in the hidden layer.</p>
<ul>
<li>All Boolean functions with <em>N</em> inputs can be represented by a perceptron with one hidden layer.</li>
</ul>
<p>True</p>
<ul>
<li>How the weights are initialised for backpropagation does not matter (provided that they are not all zero) because one usually iterates for many epochs so that the initial conditions are forgotten.</li>
</ul>
<p><strong>False</strong>. If the weights are large, then the gradient goes to zero.</p>
<ul>
<li>Nesterov’s accelerated gradient scheme is often more efficient than the simple momentum scheme because the weighting factor of the momentum term increases as a function of iteration number.</li>
</ul>
<p><strong>False</strong>. Nesterov’s accelerated-gradient method is more efficient than the simple<br>momentum method, because the accelerated-gradient method evaluates the gradient at an extrapolated point, not at the initial point.  </p>
<ul>
<li>$L_1$-regularisation reduces small weights more than $L_2$-regularisation.</li>
</ul>
<p><strong>True</strong>. </p>
<ul>
<li>The number of $N$-dimensional Boolean functions is $2^N​$</li>
</ul>
<p><strong>False</strong>. The answer should be $2^{2^N}​$</p>
<p>尝试：单选第四个错误、单选第5个错误、45错误、345错误、35错误、</p>
<h4 id="Two-layer-perceptron"><a href="#Two-layer-perceptron" class="headerlink" title="Two-layer perceptron"></a>Two-layer perceptron</h4><p>First hidden layer:</p>
<script type="math/tex; mode=display">
V _ { j } ^ { ( 1 , \mu ) } = \tanh \left( - \theta _ { j } ^ { ( 1 ) } + \sum _ { k = 1 } ^ { 2 } w _ { j k } ^ { ( 1 ) } x _ { k } ^ { ( \mu ) } \right)</script><p>Second hidden layer:</p>
<script type="math/tex; mode=display">
V _ { i } ^ { ( 2 , \mu ) } = \tanh \left( - \theta _ { i } ^ { ( 2 ) } + \sum _ { j = 1 } ^ { M _ { 1 } } w _ { i j } ^ { ( 2 ) } V _ { j } ^ { ( 1 , \mu ) } \right)</script><p>Output:</p>
<script type="math/tex; mode=display">
O ^ { ( \mu ) } = \tanh \left( - \theta ^ { ( 3 ) } + \sum _ { i = 1 } ^ { M _ { 2 } } w _ { i } ^ { ( 3 ) } V _ { i } ^ { ( 2 , \mu ) } \right)</script><p>Classification error:</p>
<script type="math/tex; mode=display">
C = \frac { 1 } { 2 p _ { \mathrm { val } } } \sum _ { \mu = 1 } ^ { p _ { \mathrm { val } } } \left| \operatorname { sgn } \left[ O ^ { ( \mu ) } \right] - t ^ { ( \mu ) } \right|</script><h2 id="Certificate"><a href="#Certificate" class="headerlink" title="Certificate"></a>Certificate</h2><p><img src="/2019/01/16/Short-Course-on-neural-networks-Tsinghua/1554651616646.png" alt=""></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ANN/" rel="tag"># ANN</a>
          
            <a href="/tags/MATLAB/" rel="tag"># MATLAB</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/30/人人尽说抽象话/" rel="next" title="人人尽说抽象话">
                <i class="fa fa-chevron-left"></i> 人人尽说抽象话
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/01/23/如何移除百度搜索结果中的百家号/" rel="prev" title="如何移除百度搜索结果中的百家号">
                如何移除百度搜索结果中的百家号 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Ga</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#OpenTA"><span class="nav-number">1.</span> <span class="nav-text">OpenTA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Quiz"><span class="nav-number">1.1.</span> <span class="nav-text">Quiz</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Analysis-Taylor-expansion"><span class="nav-number">1.1.1.</span> <span class="nav-text">Analysis: Taylor expansion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Algebra"><span class="nav-number">1.1.2.</span> <span class="nav-text">Linear Algebra</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Homework"><span class="nav-number">1.2.</span> <span class="nav-text">Homework</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3D-Boolean-functions"><span class="nav-number">1.2.1.</span> <span class="nav-text">3D Boolean functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-separability-of-4-dimensional-Boolean-functions"><span class="nav-number">1.2.2.</span> <span class="nav-text">Linear separability of 4-dimensional Boolean functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#True-False-Questions"><span class="nav-number">1.2.3.</span> <span class="nav-text">True-False Questions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Two-layer-perceptron"><span class="nav-number">1.2.4.</span> <span class="nav-text">Two-layer perceptron</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Certificate"><span class="nav-number">2.</span> <span class="nav-text">Certificate</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ga</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
